<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="keywords" content="Sitong Wu, Sitong Wu CUHK CSE, Sitong Wu CUHK, Sitong Wu Chinese University of Hong Kong"> 
<meta name="description" content="Sitong Wu's home page">
<!-- <link href="bootstrap.min.css" rel="stylesheet" media="screen" /> -->
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="myIcon.ico">
<title>Sitong Wu</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body >

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Sitong Wu <font face="verdana"> | 吴思彤 &nbsp;</font><h1>
				</div>
                            
				<h3>PhD (1<sup>st</sup> Year)</h3>
				<p>
					Department of Computer Science and Engineering</br>
					The Chinese University of Hong Kong (CUHK)</br>
					</br>
					Email: <a href="mailto:stone-wu@link.cuhk.edu.hk">stone-wu@link.cuhk.edu.hk</a></br>
        </p>
        <p>
          <a href="https://scholar.google.com.hk/citations?hl=en&user=0ao4z_MAAAAJ"><img src="./pics/logo/google_scholar_logo2.png" height="25px"></a>
          <a href="https://stonewst.github.io/"><img src="./pics/logo/github_logo2.png" height="25px"></a>
				</p>
			</td>
			<td>
				<img src="pics/wst.png" border="0" width="150"></br>
			</td>
		<tr>
	</tbody>
</table>


<h2>Biography</h2>
<p>
  I am currently a 1<sup>st</sup> year PhD student at <a href="https://www.cse.cuhk.edu.hk/en/">Computer Science & Engineering Department</a>, <a href="http://www.cuhk.edu.hk">The Chinese University of Hong Kong (CUHK)</a>, under the supervision of <a href="http://jiaya.me/">Prof. Jiaya Jia</a>. Before that, I got the Bachelor Degree in Information Engineering at <a href="https://www.jlu.edu.cn/">Jilin University (JLU)</a> in 2019. 
</p>
<p>
  My current research revolves around efficient and data-centric AI, with a particular emphasis on exploring their applications in large language models and multi-modality models.
</p>



<h2> Selected Publications [<a href="https://scholar.google.com.hk/citations?hl=en&user=0ao4z_MAAAAJ">Google Scholar</a>]</h2>
*: equal contribution
<table id="tbPublications" width="100%" style="border-collapse:separate; border-spacing:0px 10px;">
	<tbody>
  <!-- SaCo [CVPR 2024] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/saco_cvpr2024.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training</b>
      </div>
      <div style="font-size: 13px">
          <b>Sitong Wu*</b>, Haoru Tan*, Zhuotao Tian, Yukang Chen, Xiaojuan Qi, Jiaya Jia
      </div>
      <div style="font-size: 13px">
          IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024
      </div style="font-size: 13px">
          [<a href=""><b>paper (coming soon)</b></a>|<a href=""><b>code (coming soon)</b></a>]
      </div>
    </td>
  </tr>

  <!-- MoSo [NIPS 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/moso_nips2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Data Pruning via Moving-one-Sample-out</b>
      </div>
      <div style="font-size: 13px">
          Haoru Tan*, <b>Sitong Wu*</b>, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, Xiaojuan Qi
      </div>
      <div style="font-size: 13px">
          Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023
      </div>
      <div style="font-size: 13px">
          <!-- [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/3abe23bf7e295b44369c24465d68987a-Abstract-Conference.html"><b>paper</b></a>|<a href="https://arxiv.org/abs/2310.14664v2"><b>arXiv</b></a>|<a href="https://github.com/hrtan/MoSo"><b>code</b></a>] -->
          <a href="https://arxiv.org/abs/2310.14664v2"><span class="arxiv-block">arXiv</span></a> <a href="https://github.com/hrtan/MoSo"><span class="code-block">code</span></a> 
      </div>
    </td>
  </tr>

  <!-- RegionBLIP [arXiv 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/regionblip_arxiv2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension</b>
      </div>
      <div style="font-size: 13px">
          Qiang Zhou, Chaohui Yu, Shaofeng Zhang, <b>Sitong Wu</b>, Zhibing Wang, Fan Wang
      </div>
      <div style="font-size: 13px">
          arXiv: 2308.02299
      </div>
      <div style="font-size: 13px">
          [<a href="https://arxiv.org/abs/2308.02299v1"><b>arXiv</b></a>|<a href="https://github.com/mightyzau/RegionBLIP"><b>code</b></a>]
      </div>
    </td>
  </tr>
	
  <!-- UniNeXt [ACMMM 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/uninext_acmmm2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>UniNeXt: Exploring A Unified Architecture for Vision Recognition</b>
      </div>
      <div style="font-size: 13px">
          Fangjian Lin, Jianlong Yuan, <b>Sitong Wu</b>, Fan Wang, Zhibin Wang
      </div>
      <div style="font-size: 13px">
        ACM Multimedia Conference (<b>ACM MM</b>), 2023
      </div>
      <div style="font-size: 13px">
          [<a href="https://dl.acm.org/doi/10.1145/3581783.3612260"><b>paper</b></a>|<a href="https://arxiv.org/abs/2304.13700v3"><b>arXiv</b></a>|<a href="https://github.com/jianlong-yuan/UniNeXt"><b>code</b></a>]
      </div>
    </td>
  </tr>

  <!-- AxWin [arXiv 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/axwin_arxiv2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows</b>
      </div>
      <div style="font-size: 13px">
          Fangjian Lin, Yizhe Ma, <b>Sitong Wu</b>, Long Yu, Shengwei Tian
      </div>
      <div style="font-size: 13px">
        arXiv: 2305.01280
      </div>
      <div style="font-size: 13px">
          [<a href="https://arxiv.org/abs/2305.01280v1"><b>arXiv</b></a>]
      </div>
    </td>
  </tr>
  
  <!-- PRSeg [TCSVT 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/prseg_tcsvt2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>PRSeg: A Lightweight Patch Rotate MLP Decoder for Semantic Segmentation</b>
      </div>
      <div style="font-size: 13px">
          Fangjian Lin*, Yizhe Ma*, <b>Sitong Wu</b>, Long Yu, Shengwei Tian
      </div>
      <div style="font-size: 13px">
        IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023
      </div>
      <div style="font-size: 13px">
        [<a href="https://doi.org/10.1109/TCSVT.2023.3271523"><b>paper</b></a>|<a href="https://arxiv.org/abs/2305.00671v1"><b>arXiv</b></a>]
      </div>
    </td>
  </tr>

  <!-- StructToken [TCSVT 2023] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/structtoken_tcvst2023.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>StructToken: Rethinking Semantic Segmentation with Structural Prior</b>
      </div>
      <div style="font-size: 13px">
          Fangjian Lin*, Zhanhao Liang*, <b>Sitong Wu</b>, Junjun He, Kai Chen, Shengwei Tian
      </div>
      <div style="font-size: 13px">
        IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2023
      </div>
      <div style="font-size: 13px">
        [<a href="https://doi.org/10.1109/TCSVT.2023.3252807"><b>paper</b></a>|<a href="https://arxiv.org/abs/2203.12612v6"><b>arXiv</b></a>|<a href="https://github.com/RockeyCoss/StructToken"><b>code</b></a>]
      </div>
    </td>
  </tr>

  <!-- SDC [NIPS 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/sdc_nips2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Semantic Diffusion Network for Semantic Segmentation</b>
      </div>
      <div style="font-size: 13px">
          Haoru Tan*, <b>Sitong Wu*</b>, Jimin Pi
      </div>
      <div style="font-size: 13px">
        Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022
      </div>
      <div style="font-size: 13px">
        [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/396446770f5e8496ca1feb02079d4fb7-Abstract-Conference.html"><b>paper</b></a>|<a href="https://arxiv.org/abs/2302.02057v1"><b>arXiv</b></a>|<a href="https://github.com/stonewst/Semantic-Diffusion-Networks"><b>code</b></a>]
      </div>
    </td>
  </tr>

  <!-- Pale [AAAI 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/pale_aaai2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention</b>
      </div>
      <div style="font-size: 13px">
          <b>Sitong Wu</b>, Tianyi Wu, Haoru Tan, Guodong Guo
      </div>
      <div style="font-size: 13px">
          Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2022, <font color = 'red'>Oral</font>
      </div>
      <div style="font-size: 13px">
          [<a href="https://doi.org/10.1609/aaai.v36i3.20176"><b>paper</b></a>|<a href="https://arxiv.org/abs/2112.14000v1"><b>arXiv</b></a>|<a href="https://github.com/BR-IDL/PaddleViT"><b>code</b></a>]
      </div>
    </td>
  </tr>

  <!-- STM_empirical [arXiv 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/stm-empirical_arxiv2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Demystify Transformers & Convolutions in Modern Image Deep Networks</b>
      </div>
      <div style="font-size: 13px">
          Xiaowei Hu*, Min Shi*, Weiyun Wang*, <b>Sitong Wu*</b>, Linjie Xing, Wenhai Wang, Xizhou Zhu, Lewei Lu, Jie Zhou, Xiaogang Wang, Yu Qiao, Jifeng Dai
      </div>
      <div style="font-size: 13px">
          arXiv: 2211.05781
      </div>
      <div style="font-size: 13px">
          [<a href="https://arxiv.org/abs/2211.05781v2"><b>arXiv</b></a>|<a href="https://github.com/OpenGVLab/STM-Evaluation"><b>code</b></a>]
      </div>
    </td>
  </tr>

  <!-- FSFormer [ACCV 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/fsformer_accv2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Full-scale Selective Transformer for Semantic Segmentation</b>
      </div>
      <div style="font-size: 13px">
          Fangjian Lin*, <b>Sitong Wu*</b>, Yizhe Ma, Shengwei Tian
      </div>
      <div style="font-size: 13px">
          Asian Conference on Computer Vision (<b>ACCV</b>), 2022
      </div>
      <div style="font-size: 13px">
          [<a href="https://openaccess.thecvf.com/content/ACCV2022/html/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.html"><b>paper</b></a>]
      </div>
    </td>
  </tr>

  <!-- CATrans [IJCAI 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/catrans_ijcai2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>CATrans: Context and Affnity Transformer for Few-Shot Segmentation</b>
      </div>
      <div style="font-size: 13px">
          Shan Zhang, Tianyi Wu, <b>Sitong Wu</b>, Guodong Guo
      </div>
      <div style="font-size: 13px">
          International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2022
      </div>
      <div style="font-size: 13px">
          [<a href="https://www.ijcai.org/proceedings/2022/0231"><b>paper</b></a>|<a href="https://arxiv.org/abs/2204.12817v1"><b>arXiv</b></a>]
      </div>
    </td>
  </tr>
  
  <!-- FTN [arXiv 2022] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/ftn_arxiv2022.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Fully Transformer Networks for Semantic Image Segmentation</b>
      </div>
      <div style="font-size: 13px">
          <b>Sitong Wu*</b>, Tianyi Wu*, <b>Fangjian Lin*</b>, Shengwei Tian, Guodong Guo
      </div>
      <div style="font-size: 13px">
          arXiv: 2106.04108
      </div>
      <div style="font-size: 13px">
          [<a href="https://arxiv.org/abs/2106.04108v3"><b>arXiv</b></a>|<a href="https://github.com/BR-IDL/PaddleViT"><b>code</b></a>]
      </div>
    </td>
  </tr>
  
  <!-- DPGM [AAAI 2021] -->
  <tr>
    <td><img width="250" style="padding: 2px;" src="./pics/paper/dpgm_aaai2021.png"></td>
    <td>
      <div style="font-size: 15px">
          <b>Proxy Graph Matching with Proximal Matching Networks</b>
      </div>
      <div style="font-size: 13px">
          Haoru Tan, Chuang Wang, <b>Sitong Wu</b>, Tieqiang Wang, Xuyao Zhang, Chenglin Liu
      </div>
      <div style="font-size: 13px">
          Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2021</br>
      </div>
      <div style="font-size: 13px">
          [<a href="https://doi.org/10.1609/aaai.v35i11.17179"><b>paper</b></a>]
      </div>
    </td>
  </tr>

  <!--  [IJCV 2024] -->


  </tbody>
</table>







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>
</body>
</html>
