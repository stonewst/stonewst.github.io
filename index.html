<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!-- <meta name="viewport" content="width=device-width, initial-scale=1.0" /> -->
<meta name="keywords" content="Sitong Wu, Sitong Wu CUHK CSE, wu sitong, sitong wu, Sitong Wu CUHK, Sitong Wu Chinese University of Hong Kong"> 
<meta name="description" content="Sitong Wu's home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="myIcon.ico">
<title>Sitong Wu</title>
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body >

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Sitong Wu <font face="verdana"> | 吴思彤 &nbsp;</font><h1>
				</div>
                            
				<h3>PhD (1<sup>st</sup> Year)</h3>
				<p>
					Department of Computer Science and Engineering</br>
					The Chinese University of Hong Kong (CUHK)</br>
					</br>
					Email: <a href="mailto:stone-wu@link.cuhk.edu.hk" class="urltext">stone-wu@link.cuhk.edu.hk</a></br>
        </p>
        <p>
          <a href="https://scholar.google.com.hk/citations?hl=en&user=0ao4z_MAAAAJ"><img src="./pics/logo/google_scholar_logo2.png" height="25px"></a>
          <a href="https://stonewst.github.io/"><img src="./pics/logo/github_logo2.png" height="25px"></a>
				</p>
			</td>
			<td>
				<img src="pics/wst.png" border="0" width="150"></br>
			</td>
		<tr>
	</tbody>
</table>


<h2>Biography</h2>
<p>
  I am currently a 1<sup>st</sup> year PhD student at <a href="https://www.cse.cuhk.edu.hk/en/" class="urltext">Computer Science & Engineering Department</a>, <a href="http://www.cuhk.edu.hk" class="urltext">The Chinese University of Hong Kong (CUHK)</a>, under the supervision of <a href="http://jiaya.me/" class="urltext">Prof. Jiaya Jia</a>. Before that, I got the Bachelor degree in Information Engineering at <a href="https://www.jlu.edu.cn/" class="urltext">Jilin University (JLU)</a>. 
</p>
<p>
  My current research revolves around efficient and data-centric AI, with a particular emphasis on exploring their applications in large language models and multi-modality models.
</p>



<h2> Selected Publications [<a href="https://scholar.google.com.hk/citations?hl=en&user=0ao4z_MAAAAJ" class="urltext">Google Scholar</a>]</h2>
*: equal contribution
<table id="tbPublications" width="100%" style="border-collapse:separate; border-spacing:0px 10px;">
	<tbody>
  <!-- SaCo [CVPR 2024] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/saco_cvpr2024.png"></td>
    <td>
        <span class="papertitle">SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training</span></br>
        <span class="authorlist"><b>Sitong Wu*</b>, Haoru Tan*, Zhuotao Tian, Yukang Chen, Xiaojuan Qi, Jiaya Jia</span></br>
        <span class="papervenue">IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</span></br>
        <!-- [<a href=""><b>paper (coming soon)</b></a>|<a href=""><b>code (coming soon)</b></a>] -->
        <a href=""><span class="comingsoonbutton">coming soon</span></a>
    </td>
  </tr>

  <!-- MoSo [NIPS 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/moso_nips2023.png"></td>
    <td>
      <span class="papertitle">Data Pruning via Moving-one-Sample-out</span></br>
      <span class="authorlist">Haoru Tan*, <b>Sitong Wu*</b>, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, Xiaojuan Qi</span></br>
      <span class="papervenue">Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023</span></br>
      <!-- [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/3abe23bf7e295b44369c24465d68987a-Abstract-Conference.html"><b>paper</b></a>|<a href="https://arxiv.org/abs/2310.14664v2"><b>arXiv</b></a>|<a href="https://github.com/hrtan/MoSo"><b>code</b></a>] -->
      <a href="https://arxiv.org/abs/2310.14664v2"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/hrtan/MoSo"><span class="codebutton">code</span></a> 
    </td>
  </tr>

  <!-- Pale [AAAI 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/pale_aaai2022.png"></td>
    <td>
      <span class="papertitle">Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention</span></br>
      <span class="authorlist"><b>Sitong Wu</b>, Tianyi Wu, Haoru Tan, Guodong Guo</span></br>
      <span class="papervenue">Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2022, <font color = 'red'><b>Oral</b></font></span></br>
      <!-- [<a href="https://doi.org/10.1609/aaai.v36i3.20176"><b>paper</b></a>|<a href="https://arxiv.org/abs/2112.14000v1"><b>arXiv</b></a>|<a href="https://github.com/BR-IDL/PaddleViT"><b>code</b></a>] -->
      <a href="https://doi.org/10.1609/aaai.v36i3.20176"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2112.14000v1"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/BR-IDL/PaddleViT"><span class="codebutton">code</span></a> 
    </td>
  </tr>

  <!-- SDC [NIPS 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/sdc_nips2022.png"></td>
    <td>
      <span class="papertitle">Semantic Diffusion Network for Semantic Segmentation</span></br>
      <span class="authorlist">Haoru Tan*, <b>Sitong Wu*</b>, Jimin Pi</span></br>
      <span class="papervenue">Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022</span></br>
      <!-- [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/396446770f5e8496ca1feb02079d4fb7-Abstract-Conference.html"><b>paper</b></a>|<a href="https://arxiv.org/abs/2302.02057v1"><b>arXiv</b></a>|<a href="https://github.com/stonewst/Semantic-Diffusion-Networks"><b>code</b></a>] -->
      <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/396446770f5e8496ca1feb02079d4fb7-Abstract-Conference.html"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2302.02057v1"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/stonewst/Semantic-Diffusion-Networks"><span class="codebutton">code</span></a> 
    </td>
  </tr>

  <!-- STM_empirical [arXiv 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/stm-empirical_arxiv2022.png"></td>
    <td>
      <span class="papertitle">Demystify Transformers & Convolutions in Modern Image Deep Networks</span></br>
      <span class="authorlist">Xiaowei Hu*, Min Shi*, Weiyun Wang*, <b>Sitong Wu*</b>, Linjie Xing, Wenhai Wang, Xizhou Zhu, Lewei Lu, Jie Zhou, Xiaogang Wang, Yu Qiao, Jifeng Dai</span></br>
      <span class="papervenue"><b>arXiv</b>: 2211.05781</span></br>
      <!-- [<a href="https://arxiv.org/abs/2211.05781v2"><b>arXiv</b></a>|<a href="https://github.com/OpenGVLab/STM-Evaluation"><b>code</b></a>] -->
      <a href="https://arxiv.org/abs/2211.05781v2"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/OpenGVLab/STM-Evaluation"><span class="codebutton">code</span></a> 
    </td>
  </tr>

  <!-- FTN [arXiv 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/ftn_arxiv2022.png"></td>
    <td>
      <span class="papertitle">Fully Transformer Networks for Semantic Image Segmentation</span></br>
      <span class="authorlist"><b>Sitong Wu*</b>, Tianyi Wu*, Fangjian Lin*, Shengwei Tian, Guodong Guo</span></br>
      <span class="papervenue"><b>arXiv</b>: 2106.04108</span></br>
      <!-- [<a href="https://arxiv.org/abs/2106.04108v3"><b>arXiv</b></a>|<a href="https://github.com/BR-IDL/PaddleViT"><b>code</b></a>] -->
      <a href="https://arxiv.org/abs/2106.04108v3"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/BR-IDL/PaddleViT"><span class="codebutton">code</span></a>
    </td>
  </tr>

  <!-- FSFormer [ACCV 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/fsformer_accv2022.png"></td>
    <td>
      <span class="papertitle">Full-scale Selective Transformer for Semantic Segmentation</span></br>
      <span class="authorlist">Fangjian Lin*, <b>Sitong Wu*</b>, Yizhe Ma, Shengwei Tian</span></br>
      <span class="papervenue">Asian Conference on Computer Vision (<b>ACCV</b>), 2022</span></br>
      <!-- [<a href="https://openaccess.thecvf.com/content/ACCV2022/html/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.html"><b>paper</b></a>] -->
      <a href="https://openaccess.thecvf.com/content/ACCV2022/html/Lin_Full-scale_Selective_Transformer_for_Semantic_Segmentation_ACCV_2022_paper.html"><span class="paperbutton">paper</span></a>
    </td>
  </tr>

  <!-- RegionBLIP [arXiv 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/regionblip_arxiv2023.png"></td>
    <td>
      <span class="papertitle">RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension</span></br>
      <span class="authorlist">Qiang Zhou, Chaohui Yu, Shaofeng Zhang, <b>Sitong Wu</b>, Zhibing Wang, Fan Wang</span></br>
      <span class="papervenue"><b>arXiv</b>: 2308.02299</span></br>
      <!-- [<a href="https://arxiv.org/abs/2308.02299v1"><b>arXiv</b></a>|<a href="https://github.com/mightyzau/RegionBLIP"><b>code</b></a>] -->
      <a href="https://arxiv.org/abs/2308.02299v1"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/mightyzau/RegionBLIP"><span class="codebutton">code</span></a>
    </td>
  </tr>
    
  <!-- EQAN [IJCV 2024] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/eqan_ijcv2024.png"></td>
    <td>
      <span class="papertitle">Ensemble Quadratic Assignment Network for Graph Matching</span></br>
      <span class="authorlist">Haoru Tan, Chuang Wang, <b>Sitong Wu</b>, Xuyao Zhang, Fei Yin, Chenglin Liu</span></br>
      <span class="papervenue">International Journal on Computer Vision (<b>IJCV</b>), 2024</span></br>
      <!-- [<a href="https://arxiv.org/abs/2403.06457"><b>arXiv</b></a>] -->
      <a href="https://arxiv.org/abs/2403.06457"><span class="arxivbutton">arXiv</span></a>
    </td>
  </tr>

  <!-- DPGM [AAAI 2021] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/dpgm_aaai2021.png"></td>
    <td>
      <span class="papertitle">Proxy Graph Matching with Proximal Matching Networks</span></br>
      <span class="authorlist">Haoru Tan, Chuang Wang, <b>Sitong Wu</b>, Tieqiang Wang, Xuyao Zhang, Chenglin Liu</span></br>
      <span class="papervenue">Association for the Advancement of Artificial Intelligence (<b>AAAI</b>), 2021</span></br>
      <!-- [<a href="https://doi.org/10.1609/aaai.v35i11.17179"><b>paper</b></a>] -->
      <a href="https://doi.org/10.1609/aaai.v35i11.17179"><span class="paperbutton">paper</span></a>
    </td>
  </tr>

  <!-- UniNeXt [ACMMM 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/uninext_acmmm2023.png"></td>
    <td>
      <span class="papertitle">UniNeXt: Exploring A Unified Architecture for Vision Recognition</span></br>
      <span class="authorlist">Fangjian Lin, Jianlong Yuan, <b>Sitong Wu</b>, Fan Wang, Zhibin Wang</span></br>
      <span class="papervenue">ACM Multimedia Conference (<b>ACM MM</b>), 2023</span></br>
      <!-- [<a href="https://dl.acm.org/doi/10.1145/3581783.3612260"><b>paper</b></a>|<a href="https://arxiv.org/abs/2304.13700v3"><b>arXiv</b></a>|<a href="https://github.com/jianlong-yuan/UniNeXt"><b>code</b></a>] -->
      <a href="https://dl.acm.org/doi/10.1145/3581783.3612260"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2304.13700v3"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/jianlong-yuan/UniNeXt"><span class="codebutton">code</span></a> 
    </td>
  </tr>
  
  <!-- StructToken [TCSVT 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/structtoken_tcvst2023.png"></td>
    <td>
      <span class="papertitle">StructToken: Rethinking Semantic Segmentation with Structural Prior</span></br>
      <span class="authorlist">Fangjian Lin*, Zhanhao Liang*, <b>Sitong Wu</b>, Junjun He, Kai Chen, Shengwei Tian</span></br>
      <span class="papervenue">IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2023</span></br>
      <!-- [<a href="https://doi.org/10.1109/TCSVT.2023.3252807"><b>paper</b></a>|<a href="https://arxiv.org/abs/2203.12612v6"><b>arXiv</b></a>|<a href="https://github.com/RockeyCoss/StructToken"><b>code</b></a>] -->
      <a href="https://doi.org/10.1109/TCSVT.2023.3252807"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2203.12612v6"><span class="arxivbutton">arXiv</span></a> <a href="https://github.com/RockeyCoss/StructToken"><span class="codebutton">code</span></a> 
    </td>
  </tr>

  <!-- PRSeg [TCSVT 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/prseg_tcsvt2023.png"></td>
    <td>
      <span class="papertitle">PRSeg: A Lightweight Patch Rotate MLP Decoder for Semantic Segmentation</span></br>
      <span class="authorlist">Fangjian Lin*, Yizhe Ma*, <b>Sitong Wu</b>, Long Yu, Shengwei Tian</span></br>
      <span class="papervenue">IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2023</span></br>
      <!-- [<a href="https://doi.org/10.1109/TCSVT.2023.3271523"><b>paper</b></a>|<a href="https://arxiv.org/abs/2305.00671v1"><b>arXiv</b></a>] -->
      <a href="https://doi.org/10.1109/TCSVT.2023.3271523"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2305.00671v1"><span class="arxivbutton">arXiv</span></a>
    </td>
  </tr>

  <!-- CATrans [IJCAI 2022] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/catrans_ijcai2022.png"></td>
    <td>
      <span class="papertitle">CATrans: Context and Affnity Transformer for Few-Shot Segmentation</span></br>
      <span class="authorlist">Shan Zhang, Tianyi Wu, <b>Sitong Wu</b>, Guodong Guo</span></br>
      <span class="papervenue">International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), 2022</span></br>
      <!-- [<a href="https://www.ijcai.org/proceedings/2022/0231"><b>paper</b></a>|<a href="https://arxiv.org/abs/2204.12817v1"><b>arXiv</b></a>] -->
      <a href="https://www.ijcai.org/proceedings/2022/0231"><span class="paperbutton">paper</span></a> <a href="https://arxiv.org/abs/2204.12817v1"><span class="arxivbutton">arXiv</span></a>
    </td>
  </tr>
  
  <!-- AxWin [arXiv 2023] -->
  <tr>
    <td><img width="280" style="padding: 15px;" src="./pics/paper/axwin_arxiv2023.png"></td>
    <td>
      <span class="papertitle">AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows</span></br>
      <span class="authorlist">Fangjian Lin, Yizhe Ma, <b>Sitong Wu</b>, Long Yu, Shengwei Tian</span></br>
      <span class="papervenue"><b>arXiv</b>: 2305.01280</span></br>
      <!-- [<a href="https://arxiv.org/abs/2305.01280v1"><b>arXiv</b></a>] -->
      <a href="https://arxiv.org/abs/2305.01280v1"><span class="arxivbutton">arXiv</span></a> 
    </td>
  </tr>

  </tbody>
</table>



<h2>Experiences</h2>
<ul>
  <!-- Baidu Research -->
  <table width="100%" align="center" border="0" cellpadding="10">
      <tbody><tr>
        <td width="20%" align="center">
          <img src="./pics/affiliation/baidu_research.png" alt="face" width="80%">
        </td>
        <td width="80%" valign="center">
            <span class="internaffiliation"><b>Baidu Research</b></span>   &nbsp; <span class="intern_discription">(2021 - 2022)</span> </br>
            <span class="interndiscription">Research Internship | Supervisor:</span> <a href="https://scholar.google.com/citations?user=f2Y5nygAAAAJ" class="urltext2">Prof. Guodong Guo</a> <span class="interndiscription">and</span> <a href="https://scholar.google.com/citations?user=z5SPCmgAAAAJ" class="urltext2">Dr. Jingdong Wang</a>
        </td>
      </tr>
      </tbody>
  </table>
  <!-- SHlab -->
  <table width="100%" align="center" border="0" cellpadding="10">
      <tbody><tr>
        <td width="20%" align="center">
          <img src="./pics/affiliation/shlab.png" alt="face" width="80%">
        </td>
        <td width="80%" valign="center">
            <span class="internaffiliation"><b>Shanghai AI Laboratory</b></span>  &nbsp; <span class="intern_discription">(2022)</span> </br>
            <span class="interndiscription">Research Internship | Supervisor:</span> <a href="https://xw-hu.github.io/" class="urltext2">Dr. Xiaowei Hu</a> <span class="interndiscription">and</span> <a href="https://jifengdai.org/" class="urltext2">Prof. Jifeng Dai</a>
        </td>
      </tr>
      </tbody>
  </table>
  <!-- Damo -->
  <table width="100%" align="center" border="0" cellpadding="10">
    <tbody><tr>
      <td width="20%" align="center">
        <img src="./pics/affiliation/damo.png" alt="face" width="80%">
      </td>
      <td width="80%" valign="center">
            <span class="internaffiliation"><b>Alibaba Damo Academy</b></span>   &nbsp; <span class="intern_discription">(2023)</span> </br>
            <span class="interndiscription">Research Internship | Supervisor:</span> <a href="https://www.linkedin.com/in/fan-wang-b9605413/" class="urltext2">Dr. Fan Wang</a></a>
      </td>
    </tr>
    </tbody>
  </table>

</ul>



<h2>Honors and Awards</h2>
<ul>
	<li>
		  Postgraduate Scholarship, CUHK  &nbsp; (2023-2027)
  </li>
  <li>
      Outstanding Internship Award, Baidu Research  &nbsp; (2021)
  </li>
  <li>
      National Scholarship, China  &nbsp; (2018)
  </li>
  <li>
      Outstanding Student, Jilin University  &nbsp; (2016 - 2019)
  </li>
  <li>
      First Class Scholarship, Jilin University  &nbsp; (2015 - 2019)
  </li>
  <li>
      Gold Medal, Kaggle Competition  &nbsp; (2020) 
  </li>
</ul>





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>
</body>
</html>
